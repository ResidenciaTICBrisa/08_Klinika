{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Consultas a intelig\u00eancia artificial por comando de voz","text":"<p>O presente c\u00f3digo visa implementar uma solu\u00e7\u00e3o que permite aos usu\u00e1rios realizar consultas ao chat GPT-4 utilizando comandos de voz. Essa funcionalidade \u00e9 obtida ao integrar uma solu\u00e7\u00e3o existente de grava\u00e7\u00e3o e transcri\u00e7\u00e3o de \u00e1udio para texto (ver refer\u00eancias na se\u00e7\u00e3o de bibliografia). Para alcan\u00e7ar esse objetivo, a linguagem de programa\u00e7\u00e3o escolhida \u00e9 o Python, amplamente reconhecida por sua versatilidade e facilidade de uso. Essa funcionalidade faz uso dos modelos avan\u00e7ados de intelig\u00eancia artificial Whisper e GPT-4, divulgados e disponibilizados pela OpenAI.</p> <p>Para a utiliza\u00e7\u00e3o dessa funcionalidade \u00e9 preciso rodar os seguintes comandos para uso das bibliotecas:</p> <pre><code>pip install openai\npip install os\npip install pyaudio\npip install wave\npip install load_dotenv\npip install requests\npip install json\n</code></pre> <p>Inicialmente, \u00e9 necess\u00e1rio importar as bibliotecas instaladas e em seguida definir a chave de API do OpenAI na vari\u00e1vel OPENAI_API_KEY, que \u00e9 armazenada de forma segura em um arquivo de ambiente (.env). Essa chave \u00e9 utilizada para autenticar as solicita\u00e7\u00f5es \u00e0 API do OpenAI.</p> <pre><code>import openai\nimport os\nimport pyaudio\nimport wave\nfrom dotenv import load_dotenv\nimport requests\nimport json\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nopenai.api_key = OPENAI_API_KEY\n</code></pre> <p>A fun\u00e7\u00e3o record_audio \u00e9 respons\u00e1vel por gravar o \u00e1udio do dispositivo por um determinado per\u00edodo de tempo, utilizando a biblioteca \"PyAudio\". Os par\u00e2metros como formato de \u00e1udio, n\u00famero de canais, taxa de amostragem e tamanho do \"buffer\" s\u00e3o configurados nessa fun\u00e7\u00e3o. Veja que o valor padr\u00e3o para a dura\u00e7\u00e3o, caso n\u00e3o informada na chamada da fun\u00e7\u00e3o, \u00e9 5 segundos.</p> <pre><code>def record_audio(filename, duration=5):\nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 16000\nCHUNK = 1024\naudio = pyaudio.PyAudio()\nstream = audio.open(format=FORMAT, channels=CHANNELS,\nrate=RATE, input=True,\nframes_per_buffer=CHUNK)\nprint(\"Recording...\")\nframes = []\nfor _ in range(0, int(RATE / CHUNK * duration)):\ndata = stream.read(CHUNK)\nframes.append(data)\nprint(\"Finished recording\")\nstream.stop_stream()\nstream.close()\naudio.terminate()\nwith wave.open(filename, 'wb') as wf:\nwf.setnchannels(CHANNELS)\nwf.setsampwidth(audio.get_sample_size(FORMAT))\nwf.setframerate(RATE)\nwf.writeframes(b''.join(frames))\n</code></pre> <p>Ap\u00f3s a grava\u00e7\u00e3o do \u00e1udio, a fun\u00e7\u00e3o transcribe_audio realiza a transcri\u00e7\u00e3o do arquivo de \u00e1udio gravado utilizando o modelo de intelig\u00eancia artificial Whisper da OpenAI. A transcri\u00e7\u00e3o resultante \u00e9 retornada como texto.</p> <pre><code>def transcribe_audio(filename):\nwith open(filename, \"rb\") as audio_file:\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)\nreturn transcript[\"text\"]\n</code></pre> <p>Em seguida, a fun\u00e7\u00e3o generate_response \u00e9 utilizada para gerar a resposta do chat GPT-4 com base na pergunta ou comando do usu\u00e1rio. \u00c9 realizada uma solicita\u00e7\u00e3o HTTP POST para a API do OpenAI, fornecendo o modelo GPT-4 e a mensagem do usu\u00e1rio. A resposta obtida \u00e9 processada e a mensagem de sa\u00edda \u00e9 retornada.</p> <pre><code>def generate_response(question):\nheaders = {\n\"Content-Type\": \"application/json\",\n\"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n}\ndata = {\n\"model\": \"gpt-4\",\n\"messages\": [{\"role\": \"user\", \"content\": f\"{question}\"}],\n\"temperature\": 0.7\n}\nresponse = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=data)\nresult = response.json()\nreturn result[\"choices\"][0][\"message\"][\"content\"]\n</code></pre> <p>A fun\u00e7\u00e3o principal main coordena a execu\u00e7\u00e3o do c\u00f3digo. Primeiro, o \u00e1udio \u00e9 gravado e salvo em um arquivo chamado \"recorded_audio.wav\". Em seguida, o \u00e1udio \u00e9 transcrito e impresso na tela para verifica\u00e7\u00e3o. Posteriormente, a fun\u00e7\u00e3o generate_response \u00e9 chamada, passando a transcri\u00e7\u00e3o como entrada, e a resposta do chat GPT-4 \u00e9 impressa na tela.</p> <pre><code>def main():\naudio_filename = \"recorded_audio.wav\"\nrecord_audio(audio_filename)\ntranscription = transcribe_audio(audio_filename)\nprint(\"Transcription:\", transcription)\nresponse = generate_response(transcription)\nprint(response)\nif __name__ == \"__main__\":\nmain()\n</code></pre> <p>Ao executar o c\u00f3digo, \u00e9 poss\u00edvel interagir com o chat GPT-4 atrav\u00e9s de comandos de voz, fornecendo perguntas ou instru\u00e7\u00f5es. O assistente virtual processar\u00e1 o \u00e1udio, converter\u00e1 em texto, enviar\u00e1 a consulta para o modelo GPT-4 e retornar\u00e1 a resposta gerada.</p> <p>Para mais informa\u00e7\u00f5es consulte a documenta\u00e7\u00e3o da OpenAI.</p>"},{"location":"#bibliografia","title":"Bibliografia","text":"<p>SEBASTIAN. Voice to Text Made Easy: Implementing a Python App with OpenAI\u2019s Whisper Speech-to-Text API. Medium. Dispon\u00edvel em: https://medium.com/codingthesmartway-com-blog/voice-to-text-made-easy-implementing-a-python-app-with-openais-whisper-speech-to-text-api-e8f415a5f737. Acesso em: 20 jul. 2023.</p> <p>\u200c</p>"}]}